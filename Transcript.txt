Understanding Concurrency, Threading, and Synchronization
Introduction: What Are You Going to Learn in This Course?
Hello, my name is Jos√©. I would like to welcome you to this course, Applying Concurrency and Multi-threading to Common Java Patterns. This course is an introduction to concurrent programming on the Java platform using the Java language. And this first module is about understanding concurrency, threading, and synchronization. What are you going to learn in this course? Well, this is of course about concurrent programming. First, I am going to talk about concurrency. Concurrency is the art of doing several things at the same time, and we are going to define precisely what does this word "same" mean in this context. And this context has changed from the monocore CPUs to the multicore CPUs. Second, we will see what does correct code mean in the concurrent world in the context of concurrent programming. Then, we will see how we can improve existing code or new code by leveraging multi-core CPUs. This is what concurrent programming is about, improving the performance of our code. We will see how to write concurrent code, how to implement patterns, and mainly we are going to talk about the singleton pattern. This course is an introduction to concurrent programming, as I said, and we are going to talk about very important notions in this field: Race condition, synchronization, volatility, and also, more advanced topics, visibility, false sharing, and happens-before link.

Agenda: What Should You Know to Follow This Course?
Who are you, what programming background should you have to follow this course? Well, this is a Java course. All the examples are taken from the Java language, using the Java API, so you should be of course, a Java developer. Basically, you need to be able to write a simple Java application using the language and the collection framework. But you do not need to have any prior knowledge about concurrency. Even if you do not know anything about multi-threading or concurrent programming, this course is for you. This is an introduction, so it does not have any kind of requirement in this field. Let us quickly browse through the agenda for this course. The first module is about understanding concurrency, threading, and synchronization, so we are going to define precisely what a thread is, what concurrent programming means, and what is synchronization. Then the second module is about implementing the producer consumer pattern using the wait notify pattern. This is a fundamental pattern to understand in the Java language, and we are going to describe it and to define it very precisely. And then we will talk about ordering reads and writes operations on a multicore CPU. What we will see is that concurrent programming has nothing to do on a monocore CPU and on a multicore CPU. And ordering reads and writes operations is really the biggest problem on multicore CPUs. And the last module is an application, is a use case description, implementing a thread safe version of a singleton pattern on a multicore CPU. It will be the occasion to show an example and to come back on all the abstract concepts we are going to describe in this course.

Definition of a Thread, Existing Thread in Java
With no further ado, let us begin this course, and let us talk about the first topic, what is a thread? Well, a thread is defined at the operating system level. And the Java language, as well as all the other languages, uses, leverages, the service that is given by the operating system. From the developer point of view, a thread is a set of instructions that I'm going to write in my application, and execute in a certain way. An application itself can be composed of several threads. I can define nearly as many threads as I want in my Java application. And different threads can be executed at the same time. And we are going to see that this notion of same time is not the same on all the CPUs we can use. The JVM itself, the Java Virtual Machine, works with several threads. There are threads for the garbage collection. There are threads for the Just In Time compiler, and other technical threads.

What Does It Mean for Tasks to Happen at the Same Time?
Let us talk a little about this notion of at the same time. What does at the same time mean? And let us take a very classical example. Suppose we are writing a text document, using your favorite text processor, and then at the same time you are writing your text document, a spell check is running, spell check is running in the background, it does not prevent you from writing your document, and it will trigger a visual signal when you have something wrong in what you have typed in. And then you might want to print the document you are typing so you are going to launch a print command, but once again, this print command will not prevent you from continuing to type your text, and will not prevent the spell checker to check what you are typing. And of course at the same time, your operating system will check regularly for your made bugs, and if you have an important email that just came in, it will trigger a notification on the screen to tell you that maybe you should check this email. All these four operations are happening at the same time. Now let us have a look at what is happening at the CPU level deep inside my computer. Let us first consider a first case. I have a CPU with only one core in it. So really, this CPU can only do one thing at a time. So first of all I am typing my text document. So on a CPU timeline, the slice will be devoted to writing this text, then the spell checker will be run, and check the text I have just typed in. Then I can type some text once again, then the spell checker once again. Then I decide to print this document, so the operating system will invoke the print service and send the document to the printer. But I can still type in some old text, run the spell checker once again, and at a given time, my email is arriving, and my operating system is notifying me that this important email needs my attention. And again, once this notification has been sent, I can continue typing my document.

Happening at the Same Time on Multicore CPU
What we see obviously is that at the CPU level, if I have only one core, nothing is really happening at the same time. Just because my monocore CPU cannot do more than one thing at the same time. So the question is, why do I have the feeling that everything is happening at the same time? Well, everything is a matter of time scale. All those little actions we saw are just happening so fast, that I have the feeling that they are happening at the same time. If all those operations are happening in a time frame of 10 milliseconds, I will not be able to tell that one is happening at a given instant, and another one at the next instant. Things are just happening fast enough for that. Now, let us consider the second case, your CPU with multiple cores. And let us see a case where my CPU has two cores. This time, my CPU is able to do two things at the same time, one on each of its cores. So the first task will be typing my text, but at the same time, my spell checker can be run by my word processor, on the other core of my CPU. And then maybe I will continue to type my text using the first core. And then the core two will be used to run the spell checker once again, and then the spell checker once more, maybe the print command will be launched using the core two of my CPU. Maybe the email notification will be launched on the first core of my CPU, then the spell checker might be launched on core one, and then maybe I will continue to type my text using the second core of my CPU. And then things are continuing in that way, without me being able to tell or to decide which core of my CPU is going to launch which task. On a multi-core CPU, things are really happening at the same time. Why, because my CPU is really able to run several tasks at the same time. And of course this has a big impact on concurrent programming, on multi-thread programming, as we are going to see it in the rest of this course.

CPU Time Sharing Using a Thread Scheduler
We saw that there is some kind of magic happening behind the scene. Who is responsible for the CPU time sharing? Who is going to tell that the CPU should be used for writing my text, or should be used to run the spell checker? Who is responsible for the CPU time sharing? Well, there is a special element called a thread-scheduler, that is going to share evenly, the CPU timeline, divided into time slices, to all the tasks that need to be run. There are three reasons for the scheduler to pause a thread, and to tell a thread, okay, now it is the time to run another thread, so you should stop running. First, the CPU resource should be shared equally among the threads, and there are sometimes very sophisticated priority stuff that are taken into account to share equally the CPU as a resource. And there are other reasons. A thread might be waiting for some more data. Think about a thread that is doing some input output, reading or writing data to a disk or to a network. We know that writing or reading from a disk is a slow process. If the CPU is very fast, it might pause a thread waiting for the data to be available. But there is another reason, a reason that we are going to see very precisely in the rest of this course, is that a thread might be waiting for another thread to do something. For instance, to release a resource.

What Is a Race Condition in Concurrent Programming?
We defined what a thread is, what it can do. We saw that creating a thread is about doing several things at the same time. Let us talk about a very important notion called race condition. What is a race condition? Well, a race condition deals with the access of data concurrently. What does it mean accessing data concurrently? It means that two different threads might be reading the same variable, the same field defined in a Java class, or the same array. And this concurrency may lead to issues, we are going to see it. A race condition occurs when two different threads are trying to read and write the same variable or the same field, at the same time. All those words are important. Several threads can be able to read the same variable at the same time, if the value of this variable does not change, it will not raise an issue. But if they are reading and writing the same variable, then it may raise a problem. And you see that this notion of same time really needs to be very precisely defined, because on a monocore, or on a multicore CPU, same time does not mean the same thing as we saw in the previous example. This concurrent reading and writing is what is called a race condition. And as we saw, same time does not mean the same thing on a single core or on a multicore CPU. And we are going to see that.

Analysis of a Race Condition in the Singleton Pattern
Now let us see an example with the very well-known singleton pattern. Most of the time the singleton pattern is written like that. The idea is to have a class that is allowed only one single instance. So this instance is told in a private static field, here called instance. The constructor of this class is made private, so that it is not possible to build this class outside of itself. And we have a get instance, public static method, that will first check if the instance of the singleton class has already been created. If it has, it is just returned, and if it has not, it created, and returned this instance. Now the question is, what is happening if two threads are calling this get instance method at the same time? Let us take a closer look at that. Suppose that the thread scheduler chose T one first. So T one is running its task on the CPU, and the thread T two is waiting for the thread scheduler to give it the hand, that is, to give it a time slice, so that T two can run its task. First, thread T one will check if the field instance is known, because this is the test written in the code. The answer is yes because instance has not been initialized yet. So it will enter the if block. And what happens here is that the thread scheduler pauses T one and gives the hand to the thread T two. The thread T two will just do exactly the same, check if the field instance is null. And the answer is still yes. Why, because thread T one has not initialized this instance field yet. So it will also enter the if block. And since it is in the if block, it can now create an instance of the singleton class, and copy that instance in the private static instance field. And at some point the thread scheduler will give the hand back to T one. And since T one is also inside the if block, it will not check if the instance field has been initialized one more time, why? Because it already did that, it will create another instance of singleton, and copy it in the private static field instance, thus erasing the instance that has been created by the thread T two. This is a well-known race condition case, and we are going to see how to prevent that.

Synchronizing Code to Prevent Race Conditions
So the question is how to prevent that? And in the Java language, the answer is very simple, it is called synchronization. By synchronizing a block of code, we are going to prevent this to happen. Synchronization prevents a block of code to be executed by more than one thread at the same time, and from a technical point of view, it will prevent the thread scheduler to give the hand to a thread that wants to execute the synchronized portion of code that has already been executed by another thread. How does it work, technically? Well it's pretty simple, we just have to add the synchronized keyword on the declaration of the method, that becomes public static synchronized singleton get instance. How does synchronization work under the hood? This is what we are going to see now. The singleton class is a class with a get instance method that we want to synchronize. If it is not, any thread can run this method freely. Synchronizing means protecting this method by a fence here, a green fence, declared with the synchronized keyword. Under the hood, the Java machine uses a special object, called a lock object, that has a key. In fact, every object in the Java language, has this key, that is used for synchronization. What does it change to our code? Well, when a thread want to enter this protected method, this protected block of code, it will make a request on this lock object, give me your key. If the lock object has the key available, it will give it to this thread, and this thread will be able to run the get instance method freely. If another thread wants to enter this synchronized block of code, it will make the same request on the lock object, but this time, the lock object has no key available for him. Why? Just because he already gave its key to the red thread. The lock object has only one single key. So the blue thread has to wait for the key to be available. At some point, the red thread will finish to run the get instance method, will give back the key to the lock object, so this lock object will be able to give the key to the blue thread and the blue thread will be allowed to run a get instance method. When it has finished running this method, it will give back the key to the lock object. This mechanism is very simple, and will prevent more than one thread to execute the get instance method at the same time.

Understanding the Lock Object in Synchronization
So for synchronization to work, we need a special technical object that will hold the key. We are going to see that this object can be made explicit in our code. In fact, every Java object can play this role. This key is defined internally in the object class, thus making it available for all the Java objects we define in our applications. It is worth noting that sometimes this key is also called a monitor in books and articles. Now we may ask the question, how do we know which object has been chosen to hold this key? Well in fact, there are several cases to consider. In our example, we put the synchronized keyword on a public static method of the singleton class. And in this case, the JVM uses the singleton class object itself. All the classes in Java are represented by objects. And in the case of a synchronized static method, the object chosen to hold the key is the class object itself. If we put the synchronized keyword on a non-static method, then in this case, the key is held by the instance of the class we are in. So a synchronized, non-static method, uses the instance it is in, as a synchronization object holding the key. A third possibility is to use a dedicated, explicit object to conduct synchronization. Here we are creating a private final object called key. It doesn't have to be an instance of a special class. In fact, the object class itself is enough. And this key object will be used as a synchronization object holding the key. So instead of synchronizing the init method, we can create the synchronized block inside this method, and pass this key object as a parameter of this synchronized keyword. And this is probably the best thing to do, because it is always a good idea to hide the object used for synchronization, whether we are in a static context or not.

Understanding Synchronization Over Multiple Methods
Now, let us have a look at some use cases and corner cases. Suppose we have a person class. Here we are looking at an instance of his class called Mary. And we declared two methods in this class, getName and getAge. And we added the synchronized keyword on the declaration of those methods. The lock object used by the JVM is the Mary object itself, that is, the instance of the class we are in. So what is going to happen if a thread wants to execute getName? Well, it will just take the key from the lock object, and remember, the lock object is the Mary object itself, thus preventing a red thread from executing getAge at the same time. Why because since we did not declare any explicit object on the synchronization of our methods, the same key is used to synchronize both methods. This might not be what we need. If we need to synchronize getName independently of getAge, then we need to create two lock objects in the person class, and synchronize the block of codes inside the methods on those two different objects. But now suppose that we have two instances of our person class, Mary and John. And once again, the getName and getAge method are synchronized using the synchronize keyword on the method declaration. It means that the instances Mary and John are used to synchronize those methods. So we have two lock objects with two keys. The thread executing the getName of the Mary instance object, does not prevent a thread from executing the getAge from the John instance object. And it will not prevent another thread to execute this same getName method on another object. You really need to keep in mind that, to understand how synchronization works, you need to identify which object is used as a lock, and what are the keys used in your application. Remember that using the synchronized keyword on a method declaration, uses an implicit lock object, which is the class object in the case of a static method, or the instance object itself in the case of a non-static method. If what we really want is to prevent two threads to execute the getName method at the same time, in all the instances of the person class, then we need our lock object to be bound not to each instance of our class, but to the class itself. So it has to be the static field of the class person itself. And in this case indeed, the blue thread executing the getName method from the Mary object will be holding the key. Thus preventing a red thread from entering the getAge method, but also a purple thread to execute the getName method of the John instance of the person class.

What Is a Reentrant Lock?
Now that we saw how synchronization work, and what is the role of locks in synchronization, let us see two more things. First reentrant locks and deadlocks. First of all, we say in Java that locks are reentrant. What does it mean for a lock to be reentrant? Suppose we have two instances of our person class, Mary and John, and we have a bunch of synchronized method in those instances. Now it turns out that the first synchronized method from the Mary instance, calls another synchronized method of the John instance, that happens to be synchronized with the same lock. Here our methods are guarded with red fences, and to enter this method, you need the red key. So a thread that is running this method from the Mary object will take the key, and at some point will need to enter the synchronized method from the John instance. If we apply strictly the rule that we told in the previous part, the red key needs to be available for this thread to enter this method. It turns out that this red key is not available. But the thread holding that key is precisely the thread that is asking for it. So this is an exception to the rule called reentrance. And this thread, since it is already holding the right key, will be allowed to run the other method. This rule is quite natural to understand, but it still has to be stated. So we say that locks are reentrant. When a thread holds a lock, it can enter a block synchronized on the lock it is holding. And this case is very frequently met in inheritance for instance.

What Is a Deadlock?
Let us see now what a deadlock is. Suppose we still have our two Mary and John instances of the person class. And we have a synchronized method that is calling another synchronized method. Now we are not in the same case as the previous one, that is the first method is synchronized using a red key, and the method called by this method is synchronised using a green key. And for some reason, this green protected method calls another method, the third one, protected also by the red key. What is going to happen in this case? The blue thread is going to take the red key, and begin to run this first method. And at the same time, the purple thread is going to take the green key, and to run the other method. At some point, the blue thread will need the green key to enter the green method, but the purple thread has it. So this blue thread has to wait. And the purple thread will arrive at the point of code where it needs the red key to continue to run. And unfortunately the red key is not available, because it is held by the blue thread. And this case is a deadlock, that is, the green key will never be released by the purple thread, so because of that, the blue thread is blocked. And since it is holding the red key needed by the purple thread, the purple thread will never continue to advance, nor the blue thread. A deadlock situation is a situation where a thread T one holds a key that is needed by another thread T two. And the deadlock is the fact that T two also holds the key needed by T one. So as long as no thread releases its key, the situation is blocked, and called a deadlock. Fortunately, the JVM is able to detect a deadlock situation, and it can log information, mainly thread stack traces, to help debug the application. But unfortunately, there is not much we can do if a deadlock situation occurs, besides rebooting the JVM.

Quick Overview of the Runnable Pattern to Launch Threads
So in the next live coding session, we are going to launch threads. And to do that we need to use the runnable pattern. We are going to see the runnable pattern in details in the next module. So just now, I would like to give you a first glimpse at this pattern, so that you can easily follow what we are going to do in the next live coding session. The most basic way to create threads in Java is to use the runnable pattern. In the runnable pattern, you need first to create an instance of the runnable interface, which is very easy, there is only one method to implement, we are going to see that. Then we pass this instance to the constructor of the thread class. And then we just call the start method of this thread object created, to launch a new thread that is going to run the task wrapped in our runnable object. So first we create an instance of a runnable. There is only one method to implement, which is called the run method. This is the Java 7 pattern to do that, with an instance of an anonymous class. But we can also use a lambda expression to implement a runnable, since there is only one method in the runnable interface. If you are not familiar with Java 8 lambda expression, you can check my other Pluralsight course called From Collection To Streams in Java 8 Using Lambda Expressions. Then we pass this runnable object to create a new thread, and call the start method of the thread instance. So this is all we need to know for the moment. We will see more details about the runnable patterns, especially the corner cases and caveats in the next module, but right now and with this, we can jump in our first live coding session.

Live Coding: Launching Your First Thread
Time now for a little live coding session. What are we going to see in this session? Well, first we are going to see some Java code, which is very nice. We are going to create threads on very simple examples. We are going to see what can go wrong with a race condition, that is with unsynchronized code that should be synchronized, and we are going to fix our code using synchronization. This first example will be very simple, it's very basic. It is just about creating a task. A task is an instance of the runnable interface, let us call it runnable, and we can implement this interface using a lambda expression. This task will be given to a new thread, and executed in the context of this thread. So we are just going to print out the name of the thread that is running this task. I am running in... Thread. currentthread. Currentthread is a static method of the thread class that returns the thread running the current task. And we just have to call getname on this thread object to return the name of a thread. So now we can create a thread, as we saw in the slides. New thread. Passing this runnable as a parameter. So this thread is going to execute this piece of code. And to launch it, T. start. This is the start method that we need to call. What we can also do is give an explicit name to this thread T. T. setname "My thread. " And now we can execute this code. And you'll see that the task we just wrote is executed in the thread "My thread. " Now, suppose we make a mistake, and instead of calling the start method, we call the run method, as we saw in the slides. If we run this code, the problem is that the task is correctly executed, but it is not executed in the thread we have created. It is executed in the main thread which is the thread executing the main method. So this run method should not be called if we want to launch a new thread. Let us just write it here. This is the start method that should be called.

Live Coding: A Race Condition in Action, and How to Fix It
For this second example, I have created a special class, Longwrapper. This Longwrapper is just a very simple class, wrapping a long value. I have a method get value that returns the current value, and increment value that just increments this value. Let us create now a race condition, using this Longwrapper class. I am going to create a Longwrapper object, = new Longwrapper, let us give it the value zero. I am going to create a runnable using a lambda expression. And this runnable is going to increment... This Longrwapper a thousand times. Let us run this runnable in a new thread. I am going to call the join method here. That throws an InterruptedException. Just to be sure that the code that I'm going to write after the call to this join method is executed once this thread has finished executing this runnable. This is just a trick here. And I am going to print out the result, value equals Longwrapper. getValue. Let us run this code. Of course the result is the expected result, 1000. Now suppose I modify this code, and instead of using just one thread to run this runnable, I am going to use a thousand threads. The code becomes the following, I have a for loop here. And I am going to create a new thread for all the content of this array, start all of them, and then join all of them to be sure that they have all executed the runnable correctly. So now I have a thousand threads, incrementing a thousand times, the value in my Longwrapper. So I expect the result to be one million. Let us execute this code. The result is not one million. And if I run it several times, I can see that, the result is not always the same. And if you run the same code on your own machine, there is no reason that you will observe the same result as me. Where does it come from? Well, it is a very classical example of a race condition. Here in the increment value method, I have a read of the l value, I increment it locally, and copy it into the same field l. So this read is followed by a write operation. So this operation is basically a read and write operation from different threads at the same time, so this is a race condition. How can I fix this code? Let us create a key object. And let us synchronize this block of code on this key object. And this time, if I run my code, I will have the correct value, which is one million.

Live Coding: A Dead Lockin Action, and How to Fix It
For this last example, I have created a class a, with two key objects, key1 and key2. And three methods, a, b, and c. A has only one block of code synchronized on key1, which just says I am in a, and I am running in this thread, and then I am going to call b. B is basically the same method as a, but it is synchronized on key2 instead of key1. And c is the same method as a, synchronized on key1, and that does not call any more method after the printing of the current thread. So here I am creating a possible deadlock, since if someone calls a and b at the same time, a will be blocked because this call will be waiting for key2 to be available, but should be held by the other thread. So to see that, I have created another class running a that just creates an instance of a. Two tasks, instances of runnable. The first one calls the a method on the a object, and the second one the b method. I have a first thread that is going to run my first runnable, and a second one that is going to run my second runnable. And I am just waiting for both thread to stop by calling the join methods. What is going to happen if I am just running this code as it is? Let us see that. I can see that Thread 0, which is the first thread I am launching, just prints I am in a, and the second one just prints I am in b. That is, the first thread is blocked, it cannot enter the b method, and the second one is also blocked, it cannot enter the c method. And you can see from this little red button here, that the JVM does not return, it cannot finish the execution of this code. Why, because there is a deadlock in it that is blocking my two threads. The only way to finish this code is just to kill the JVM by clicking this button here. Now instead of running this code like that, I am going to run it in debug mode. To have more information about what is going under the hood. Let us switch to the debug perspective of Eclipse. I can see that I have my three threads running, the main thread one and thread two. I can pause main and see that I am blocked in fact on this line, which is t1. join, that is, I am waiting for the thread t1 to complete. We have the two threads, Thread 1 and Thread 0, which are here, which correspond to t1 and t2. Let us suspend t1 first. T1 is blocked here on this operation. It cannot execute it. Why, because it is waiting for Object id=28, meaning that it is waiting for the key to this object. What is this object id 28? It should be either key1 or key2. And indeed, key1 is the object id 27. And key2 is the object id 28. So what we see is that the thread t1 cannot execute this System. out println, because it does not own the object 28, which is shown here. Now let us suspend the thread T two, and we can see that the same goes for T two. That is, T two is waiting for object 27, which is key1. So it cannot execute this System. out println, because it does not own key1 yet. And the debugger gives us more information about that. It tells that this thread one which is T two, owns key2, and that key2 is waited by t1. And it is waiting for key1. That is owned by t1. So we are here in a deadlock situation. I just showed you how you can use a debugger to check for this situation. So far, the only way to fix this, is to shut down the JVM.

Wrap-up of the Module
Now is the time to wrap up this module. What did we learn in this first module? Well, first of all, we saw that a thread is a special thing that executes a task in a special context. In Java, the thread is modeled by an object, instance of the thread class, and the task is modeled by an instance of the runnable interface. We saw an fundamental notion, the notion of race condition, that occurs when different threads are trying to read and write the same variable at the same time. We saw how to synchronize code to avoid race conditions precisely, and we saw how synchronization relies on key objects. And we saw two consequences of that, reentrant locks, and deadlocks. And in the live coding session, we saw a very important point, which is how to use the debugger of our IDE to analyze threads and sort out a deadlock situation. Thank you for your attention, let us meet in the next module of this course, about the consumer producer pattern, implemented with wait notify in Java.

Implementing the Producer/Consumer Pattern Using Wait / Notify
Introduction: Agenda of the Module
Hello, my name is Jose. Welcome to the second module of this course, module called Implementing the Producer/Consumer Pattern Using Wait and Notify. What are we going to see in this module? Well, we are going to see in details the Runnable pattern, pattern in Java used to launch and stop threads. We are going to see what is the producer/consumer pattern. This pattern is very widely used. There are several solutions to implement it in Java, and we are going to see the most simple one, the one based on the Wait/Notify pattern. So we will see how to implement this producer/consumer pattern using a synchronization and the wait/notify pattern. So, let us see now the Runnable pattern. This pattern is about launching threads, and it is the first pattern introduced in Java to do this. It has been introduced in Java 1. 0, that is in the very early days of the language and it is still used nowadays, of course. There are other patterns introduced in Java 5, in the java. util. concurrent API. We are not going to see them. They are beyond the scope of this course.

Launching a Task in a New Thread with the Runnable Pattern
The first question we are going to answer is how to launch a new thread. A thread is something that executes a task. In Java 1, the model for a task is the Runnable interface. It is a very simple interface called Runnable. There is only one method in it, run. That does not take any argument and that does not return anything. Since this interface has only one method, in Java 8, it became a functional interface with this functional interface annotation added to it. It does not change anything in Java 7. It does not introduce any kind of backward incompatibility. It is just leveraging a new feature of the Java 8 compiler. This Runnable interface can be implemented using a number expression as it is the case here. If you're not familiar with number expression, you can check my other course called, From Collections to Streams in Java 8 Using Number Expression. So first of all, the Runnable pattern consists in creating an instance of Runnable like this. Then, creating a thread object by passing this Runnable task as a parameter to the constructor of the thread class. And then, launching this thread by calling the start method of the thread object. Now, there is also a run method in this thread class. Be careful when you launch a new thread. It is the start method you want to call and not the run method. The run method will indeed call the run method of the task object you have passed as a parameter but it does not execute this call in another thread, it executes it in the current thread so this is not what you want to do. What you want to do is really create a new thread and this is what is done by calling the start method. There is a trick to knowing which thread a task is executed. There is a static method in the thread class called current thread that will return a reference to the thread that is running this task. So when I'm writing thread. currentthread. getname, what I'm doing here is printing out the name of the thread running this task.

How to Stop a Thread Using the interrupt() Method
The second question is how to stop a thread. It might look simple to answer but in fact, it is a little tricky and there are traps behind that. Stopping a thread is more tricky than it seems. There is indeed a method in the thread class called stop. If I am mentioning this method here, it is to tell you that you should not use this method at all. If you check the documentation on this method, you will have the exact reasons that this method has in fact been introduced in the first version of the thread class before the people who wrote it realized that it was a wrong idea to create such a method. The problem is once this method was published, it was not possible to remove it on the thread class without introducing a backward incompatibility. This method is here just for legacy and backward compatibility reasons. It will not be removed in future versions but the only thing you want to know with this method is that you should stay away from it. The right pattern to stop a thread is in fact to use the interrupt method. Now the trick is the interrupt method will not stop a thread, but merely send a signal to the task the thread is running telling it that it is time for this task to stop itself. So how does it work? I have a thread, whatever it is, I call the interrupt method on this running thread. The code of the task this thread is executing should call the isInterrupted method inside itself to decide to terminate itself. So in my Runnable task as I wrote previously, I need some kind of while loop and while the thread that is running the is not interrupted, then I can carry on with whatever I have to do but if the thread has been interrupted, then I should stop myself, cleaning all the resources I have opened, for instance. So how can I stop a thread? Well, I need to call interrupt on the thread I want to stop and this calling will cause the isInterrupted method to return true So this isInterrupted method should be scanned, should be regularly checked by the Runnable that is executed by that thread. If the thread is blocked or waiting, then the corresponding method will throw an interrupted exception. We have not seen in which case a thread can be blocked but there are several examples of that, for instance, the wait and notify method that we are going to see now, offering this exception and the join method that we saw in the live connect session of the previous module, also throws an InterruptedException.

Implementing a First Producer/Consumer Example
Let us talk now about the Producer/Consumer pattern. First of all, what is a producer/consumer? Well it is a very simple and very easy to understand pattern. We have a producer that is producing values stored in a buffer, think about an array for instance. And we have a consumer that is consuming the values from this buffer. Most of the time, I will have more than one producer, more than one consumer, and they all be executed in their own thread. Of course, we need to take care of the fact that the buffer can be empty or full. So if it is empty, a consumer cannot consume values. If it is full, a producer should not try to add values in it. As I said, producers and consumers are run in their own thread. It means that these buffer, which is shared among all the threads, may be the object of a race condition if I do not properly synchronize my code. This is a very simple producer. I have a count variable that will just count the number of elements present in the buffer. My buffer is just an array of int and my producer has a method produce that will check if the buffer is full. While the buffer is full, of course, it should not try to add any object in it and when there is some room in the buffer, then it will just add a value in this buffer, the one int, and then the consumer has a method consume. While the buffer is empty, the consumer should not try to consume any value from the buffer, just because there are none, and when there are values available, then it will just take a value from this buffer here, setting the one to zero.

A First Synchronized Version of the Producer/Consumer
Okay so that was the very first and very na√Øve way of writing a producer and a consumer. The first question is: What is wrong with this code? Well of course, there is one main thing that is completely wrong, is that I have quite an obvious race condition here. If my producer and my consumer are run in their own threads, it mean that I have several threads reading and writing the buffer and the count variable at the same time. This is the exact definition of the race condition we gave in the previous module of this course. In Java, the effect will be very nasty. It will corrupt the array, I will not be able to write or read values from this array because of concurrent excess on this array. So how can I fix my producer/consumer? One way to fix things here is just to synchronize the access to the array. And this is what we learned in the previous module so let us just do that. So here is our first na√Øve attempt at fixing this problem, just synchronize each consume and produce method by adsing the synchronized keyword on it. Does it really fix our problems? Well in fact it does not because synchronization will indeed be the solution of our problem but not if we write it like that. Why? Because as we said in the previous module, if we write synchronized like that, it means that the object holding the key that the thread will need to run the consume and the produce method is the consumer instance itself and what we want is to avoid a thread from running the consume method when another thread is running the produce method so we need a common synchronization object command to all the instances of consumer and producer. So we need a code that will look like this one, a private object lock that will be made common to all the consumers and producers instances and that will be used in a synchronized block inside the consume method for the consumer and the produce method for the producer. This version of the code will work if the lock object is the same for all the instances of producers and consumers. So the code, in the end, will look like this one, with a synchronized block inside the consume and the produce method.

What Is Needed to Fix This First, Flawed Version
Now, is our code really fixed? If we write it like that, will it really work? Let us take a closer look at what we have written. And the question is: What happens if the buffer is empty? Well, the thread executing this consumer will be running the isEmpty method inside this infinite while loop forever so this thread will be blocked inside the consume method, inside the synchronized block, while holding the key of the lock object. So what is going to happen in the producer? The thread running the producer will be waiting for the key held by this lock object and since this key is not available because it is held by the consumer thread, it will not be able to execute its synchronized block. So it will have no chance to add any object to the buffer. So in fact, this way of writing things, this way of naively synchronizing our two methods is is not the right way to do it, it will lead to a deadlock, not the exact same kind of deadlock we saw in the first module but still a deadlock. So we need another fix for this one. What we need to do is a way to kind of park the consumer thread while it is waiting for some data to be produced. And when this thread is parked, it should not be blocking all the other threads and of course, namely, the producer threads not are going to add data in our buffer. So the consequence of that is that the key held by the consumer thread should be released and made available to the producer threads while this consumer thread is parked. And this is exactly what does this wait and notify pattern.

Understanding How the wait() and notify() Methods Work
So how does it work, this Wait/Notify pattern? What does it do? This is what we are going to see now. From a pure technical point of view, wait and notify are two methods from the object class was available and all the Java objects we create. These methods are invoked on a given object. Now, there is a rule the thread executing the invocation should be holding the key of that object. If the thread that is executing a wait method does not hold the key of the object on which it is executing this method, then an exception is raised. Now, we must keep in mind that the only way for a thread to hold the key of an object is to be in a synchronized block, synchronized on this object. So the consequence of that is that wait and notify cannot be invoked outside a synchronized block. And if you have an invocation of these methods outside of a synchronized block, this is a bug, it will crash when you try to run that. Let us see first what is happening when a thread calls the wait method. First of all, calling the wait on a lock object releases the key held by the thread. So this key becomes available to the other thread and this is exactly what we want, as we saw in our example. The second thing it does is that it puts the current thread in a particular state called the WAIT state. This WAIT state is not the same as the state in which a thread is when it is waiting at the entrance of a synchronized block. It is a special thread state. The only way to release a thread from a WAIT state is to call notify on the lock object this thread is using. So what happens when we call notify? Calling notify released a thread that is in a WAIT state so a thread that has called a wait method and it puts it in the Runnable state. And this is the only way to release a waiting thread. So if you never call notify in your code, at some point, the new application will most probably not work properly. If there are more than one thread in the WAIT state, and this is the case most of the time, the released thread by the notify method is chosen randomly among those threads. I also have a notifyALL method that will awake all the threads in the WAIT state.

Fixing the Producer / Consumer Code Using Wait / Notify
So let us have a look at the second version of our code. I still have my produce method with a synchronized block inside. If my buffer is full, then instead of checking if it is full once again, I am just going to put this thread in the WAIT state by calling lock. wait. So at this point, the key held by the thread running this method will be released and made available for the consumers. If my buffer wasn't full, then I add objects to my buffer and since I put objects in my buffer, I am going to notify all the consumers that may be in the WAIT state. On the consumer part, how does it work? Well, I also have a synchronized block on this locked object. If my buffer is empty, then I need a producer to add an object in it so I am going to park this consumer thread in WAIT state by calling the wait method on this lock object. If it is not the case, it means that I can consume and object from my buffer and since I have made some room in my buffer, I can now notify all the producers that are in a WAIT state.

Understanding How Synchronization Works with wait() and notify()
Let us explain that in more details. So I have a buffer that is empty for the moment. I have this wait list associated to my lock object. And this lock object has a key which is green. I have two instances of consumer and producer, with a consume and produce method guarded by the same key here in green. So let us consider an orange thread that is trying to run my consumer. This orange thread is entering the synchronize block so it is going to take the key from my lock object. At the same time, the purple thread is trying to run my producer but it cannot enter this synchronized block because the key is not available. At some point, my consumer thread will try to consume an object from my buffer but this buffer is empty for the moment. So since this buffer is empty, it will call the wait method on this lock object. The wait method has two consequences. First, it is going to park my orange thread in the wait list and release the green key. And then the key becomes available for the purple thread to take it so it is going to take it, run the produce method from my producer, at some point, create some data and add this data to the buffer, and then call the notify or notifyALL method, of this lock object. It has the consequence of taking my orange thread outside of the wait list, making it available to run again in my consume method. At some point, the purple thread will exit the producing method, releasing the green key, giving the chance to my orange thread to get the key and continue to run the consume method where it was. At this point, it is going to take the object that has been added to the buffer, then continue to run the consume method, exit this method, thus releasing the key. This is exactly how the consumer/producer pattern works with the wait/notify pattern in Java.

Live Coding: Unsynchronized, Flawed Producer/Consumer in Action
What are we going to see in this live coding session? Well first of all, we are going to see some code, of course, and we are going to see this producer/consumer pattern in action with the wait/notify that we just saw. Let us see now, this wait/notify pattern on the producer/consumer. So what do we have here? We have a producer/consumer class with our buffer and our counter just as in the slide. We then have member class producer with one single method produce. Produce is just checking if the buffer is full and if the buffer is not full, it's just a data one inside this buffer. Then we have the same kind of class here, consumer, with only one method, consume. If the buffer is empty, it just waits on some data to be added in this buffer and it will just consume one element of data once there is one available. And then we have our main method, so we just initialize the buffer with 10 elements, count to zero, create a producer and a consumer. Then we create two tasks, one produceTask instance of the Runnable interface. It is going to produce 50 elements inside this for loop and we'll just print a message once this is done, Done producing. And the same goes for the consume task. The consume task is going to consume the 50 elements produced by our produceTask. And then, we have a consumerThread that is created on the consumeTask and a producerThread that is created on the produceTask. We launch our two threads here and here and then wait for them to complete using the join method. And at the end, we just show what data is left in the buffer by printing out the value of count. Of course, we expect in this case, count to be equal to zero so we expect the message Data in buffer zero, once we have run this code. So let us run it and check what is happening. Well, our producer is done producing so it has produced 50 elements. Our consumer is also done consuming so it has consumed 50 elements. And the magic is there are seven elements left in our buffer. What is the reason for that? Well the reason for that is, of course, a race condition. We have a races condition here, absolutely everywhere on the buffer and on the count object here.

Live Coding: Synchronized, Flawed Producer/Consumer in Action
So one first way of fixing this race condition is to naively synchronize everything. Let us do that. We need a lock object here. Private static object lock object. And we're going to use it to synchronize the production and consumption of our element so here, synchronize them. Lock. And synchronize on consume. Let us run this code again. This time, it does not go too well because the JVM does not terminate. We don't have the message done producing or done consuming, meaning that the threads are still running. They are probably locked somewhere. So let us kill this and run it now in debug mode. So we are still locked. Let us switch to the debug perspective on eclipse. Let us see where our main thread is blocked. Well, it is blocked on the joining of the consumer so obviously, the consumer is still running. Let us release the thread. Let us post thread zero, which would be the consumer thread and indeed it is. We can see that this thread is owning object ID 27, which is obviously our lock. And it's probably running again and again, this is empty buffer code here. Let us see what is happening for the other thread. The producing thread. Well, this thread seems to be on the same method but the difference is that it is waiting for the object so it is not executing this line in fact, it is waiting for the key of the lock object and cannot execute while it's full buffer. Why can't it execute this code? Well, because the key off the lock is held by the consumer that will never release it since the buffer cannot be filled by the producer.

Live Coding: wait/notify, Correct Producer/Consumer in Action
So let us kill this JVM. This solution does not work. The right solution is different from this one. Inside the synchronize block, if the buffer is full here, then we are going to park this thread, releasing the lock. So lock. wait here. This wait method throws an interrupted exception so let us catch it like this. And once we've added some data in our buffer, we need to notify the threads that are waiting by calling the notify method. And we are going to do the same for the consumer so instead of a while here, we are executing an if. If the buffer is empty, then I need to wait for a producer to add some data in it. Let us handle the exception properly and notify once we have consumed a data. Let us run this code again and this time, the data left in the buffer is indeed zero, which shows that our system is working properly. We can modify it a little by, for instance, consuming only 45 items. Let us run it again. Indeed, there are five items left in the buffer, which is the expected result.

Understanding the States of a Thread
We mentioned several times the state of the thread but we did not define what it is exactly so let us do this now. A thread has a state, as we saw it. A thread can be running or not, which is already a first glimpse at what the state of a thread could be. The question is: If a thread is not running, can the thread scheduler give it the hand? That is, can the thread scheduler awake the thread and give it a time slice so that the thread can run its task? This question is interesting because it has, in fact, several answers. In most of the time, the answer is yes but we saw several cases in which the answer is no and namely, if the thread is in the wait list, then the thread scheduler should not try to give this thread a hand and a time slice to run its task. So the answer to this question is not that simple. It's not that obvious, and we need to know the exact different states a thread can have and what are the relationships between those states. So basically, we need to have a look at the state diagram to understand this exactly.

Understanding the State Diagram of a Thread
The first state we have is the state New. When we create a thread by thread T equals new thread of some task, this thread is in the New state. It has not been run yet. It has not executed its task yet. The second state is the Runnable state. Once we call the start method on this thread, the thread is set in the Runnable state. It means that the thread scheduler is free to give a time slice of the CPU to the thread so that this thread can execute its task. And once the task is completed, this thread enters the Terminated state, in which the thread scheduler knows that the thread should not be run anymore. This is the basic state machine of the different states of a thread but as we saw, there are other states in which a thread can be. The first one is the Blocked state. When a thread is blocked at the entrance of a synchronized block because the key of the lock object is not available, it is in a Blocked state and the thread scheduler will not try to awake this thread as long as the key is not available. This is a very common state in which threads can be put. The second state is the Waiting state. We also saw the Waiting state. It is a state in which the thread is put once the wait method has been called. In this case, the thread is parked in the wait list and can be awakened only by a notify call as we saw in a previous section of this module. And then we have a last state that we did not see, which is called the Timed_Waiting state. In fact, the wait method that we saw can also take a time out, which is a time out expressed in milliseconds. At the end of its time out, the thread will be automatically modified by the system and in this case, this thread will be awakened without calling the notify method on the right object. So in that case, the thread will be put in a Timed_Waiting state. It is the same state as the one the thread is put when we call the sleep method on a given thread. So the thread can go from the Runnable state to one of those three states and can go back to the Runnable state depending on the actions taken on our system.

Wrap-up on the State of a Thread
So if we take a look at what the thread scheduler can do, a thread scheduler can run the threads that are in the state Runnable. A blocked thread can only run again if the key is released. Remember that a blocked thread is blocked at the entrance of a synchronized block of code. So this block of code is guarded by a monitor that has a key. And a waiting thread can only run again when the notify method is called. It is very important to keep in mind that in the two last cases, if the key is never released or if the notify method is never called, then the blocked and the waiting threads will never be released thus, probably blocking our entire application. We can read the state of a thread using a getstate method that is present on the thread class itself. It returns an enumerated value of type Thread. State. This state enumeration is a member enumeration of the Thread class and is public so we have access to it and it defines the six states a thread can have, New, Runnable, Terminated, Blocked, Waiting, and Time_Waiting.

Wrap-up of the Module
Time to wrap up this module. What did we learn in this module? First, we saw precisely how the Runnable pattern works with the Runnable interface and the thread class. It is fairly simple to set up. We also saw how to set up a producer/consumer pattern. And we saw what can go wrong with it if the producer and the consumer are running in different threads, which is the case, of course, most of the time. And we saw how we can fix this pattern using synchronization and the proper use of the wait/notify pattern. That is all for this module. Thank you for your attention and bear with me, in the third module of this course, we are going to talk about a very important notion which is ordering reads and writes in a Java application. We are talking about the Java memory model and the happens before link.

Ordering Read and Writes Operations on a Multicore CPU
Introduction: Agenda of the Module
Hello, my name is Jose, and welcome to the third module of this course, "Ordering Read and Write Operations on a Multicore CPU". Let us quickly browse through the agenda of this module. In the previous module we saw what synchronization is and how it works in Java concurrent programming. This module is about visibility. Visibility is the second fundamental notion you need to understand in Java concurrent programming. It is very closely bound to the way multicore CPUs work, so we are going to see a little about that. We are going to see something defined in the Java Language Specification which is called the "happens before" link, and we are also going to see why it is important to understand what is an "happens before" link. We will see what the special keyword "volatile" means, how it can be used and why it should be used sometimes on the declaration of fields. We will also see the side effect of the way multicore CPUs work, and namely of the way CPU calls caches are organized, which is called "false sharing", and we will see examples on some code in Java.

Introduction to Visibility: Back to Our Producer/Consumer Case
So, first of all, let us talk about synchronization and visibility. We saw that synchronization is about protecting a block of code, which might be a whole method or some portion of a method. It guarantees that the code protected in a synchronized block is executed at one time by only one thread. So, synchronizing a block of code is about preventing two threads from executing this piece of code at the same time. It is there to prevent race condition, and race condition is when several threads are trying to read and write the same fields of a Java class at the same time. Let us take a look back at one of the use cases we saw, which is the Producer and Consumer pattern. This is a simplified version of the Producer-Consumer pattern, and there is not the Wait and Notify pattern we saw in the previous module, but this is just a simple example. And let us have a very close look at the count variable which is used to point to the available cell of our buffer. In the Consumer block of code we have a read on the count variable from memory, a decrementation, and then we write the new value of the count variable back to memory. And on the Producer side we have the same kind of pattern, we read the value of count from memory, we increment it, and then we write its new value back to memory.

Organization of Caches on Multicore CPUs
Let us talk a little more about memory access because this is precisely the key point of visibility. About 20 years ago, when CPU had no cache, this code was just working fine with no side effect, but nowadays things are different, they do not work like that any more. The CPU does not read a variable from the main memory, it reads it from its internal cache. So let us have a closer look at that. How does the CPU work with its main memory? Well, we have a first electronic component called CPU, in fact it is called CPU but there are many sub-CPUs on it called "cores". And there is the main memory, which is a different electronic component, linked to the main CPU using a special piece of electronic called a "bus". On a CPU I may have several cores, several physical cores, suppose we have four. Each core has several layers of memory caches called "L1", "L2", and a common third layer called "L3". This is a very classic pool architecture that we have on Intel CPUs for instance. Why has it been made like that? Just because access to caches is much faster than access to main memory. Access to main memory is limited by the speed of this electronic bus linking the main CPU to the main memory. Reading and writing in a cache is much faster, so it allows for much faster computation than computing directly in main memory. Access to the main memory is in the order of 100 nanoseconds. Access to the L2 cache is in the order of 7 nanoseconds, roughly 15 times faster. And access to the L1 cache is in the order of 0. 5 nanosecond, roughly 20 times faster than the access to the main memory. Of course there are tradeoffs, and the tradeoff is the amount of memory available. On the main memory I very commonly have several Giagabytes of data available, the size of the L2 cache is about 256 Kilobytes, so much less, and the size of the L1 cache is in the order of 32 Kilobytes. So I have a tiny piece of memory but with a very fast access.

Definition of Visibility on Multicore CPUs
Now that we have this architecture in mind let us have a look at what is happening with our count variable. Our Producer, which is running in Core 1 needs a copy of this count variable, so this count variable will be copied in the L1 cache of the Core 1 of our CPU. Then Core 1 can modify this variable, that is increment it. But it turns out that Core 2, which is running the Consumer, also needs the same count variable. Now, the problem comes from the fact that the count variable is really stored in two places on my CPU, first in main memory, and the value of count in main memory has not been updated yet because the write to this main memory is much, much slower than the write to the L1 cache. But my Core 2, since this variable is synchronized, should get the value 1 from the L1 cache of the Core 1 of my CPU and not the value of 0 from the main memory. So it needs a technical trick here to know that the write value is in the L1 cache of Core 1 and not in the main memory. This is exactly what visibility is. Visibility is about informing the other caches of my CPU that a variable has been modified and that the write value is in one of the caches of the CPU and should not be fetched from the main memory. So, right now, what is visibility? A variable is said visible if the writes made on it are themselves visible, which means that the reads made on this variable are going to return the correct value. The nice thing is that all the synchronized writes, that is all the modification of variables made within the boundary of a synchronized block, are visible.

Understanding Why the Happens-before Link Is So Important
And this is what this "happens before" link notion is about. In fact, the "happens before" link is an abstract notion introduced in Java that is going to help us order the read and the write operations on a multicore CPU. Let us see that now. First of all, why is it important to understand what "happens before" link is? Well, there is one main reason, is that if you check the Javadoc, and especially the Javadocs of several special classes from the java. util. concurrent package you will see that there are many references to this "happens before" link. Here are some examples from the BlockingQueue. The Javadoc tells us that actions in a thread prior to placing an object into a BlockingQueue happens before actions subsequent to the access or removal of that element from the BlockingQueue in another thread. Or in this other class, actions in a thread prior to calling await happens before actions that are part of the barrier action, this is the CyclicBarrier class. And the last example, from the CountDownLatch class, until the count reaches zero actions in a thread prior to calling countDown happen before actions following a successful return from the corresponding await in another thread. And this "happen before" notion is not explained in the Javadoc, it is explained in the Java Language Specification.

Definition of the Happens-before Link from the Java Memory Model
In this Java Language Specification there is a chapter called the Java Memory Model. What is the Java Memory Model about? Well, as we saw, multicore CPU brings new problems since a variable can be stored in multiple places, main memory and several caches. Read and writes can really happen at the same time, and this is different from the situation we used to have in monocore CPUs. A given variable can be stored in more than one place, and different threads need to be aware of that because they need to read the correct value of a given variable. Visibility simply means that a read operation should return the value set by the last write, and of course the important word of the sentence is the word "last". What does it mean the last write in a multicore CPU world? So basically we need some time of timeline to put read and write operations on, and of course a CPU does not offer such a service, so we do not have this tool exactly, we need to reproduce it to have correct operations. So, what does the Java Memory Model tell us? Suppose we have a write operation executed in a given thread T1. Here T1 just writes the value 1 to the variable x, and we have another thread, T2, which is reading this variable. It reads x and copy the value in another variable, r. So this is basically a read operation. The question is, what is the value of r once this code has been executed? In fact there are two answers to this question, and the Java Memory Model fixes those two answers. If there is no "happens before" link between the two operations then the value of r is unknown. It should be 1, which is the correct value from x, it also could be 0, which is the default initial value of x. But if there is a "happens before" link between the two operations, that is if x = 1 is set to happen before r = x then the value of r should be 1. This is the specification from the Java Language Specification, and it should be working like that in our code. Now the questions is, how can we set up a "happens before" link? Because there is no such keyword in the Java language.

Understanding the Happens-before Link on Basic Examples
The definition states that a "happens before" link exists, is created, between all synchronized or volatile write operations and all synchronized or volatile read operations that follow. We saw what a synchronized read or write is, it is simply a read or write operation within the boundary of a synchronized block of code. We didn't see what a volatile read or write is, and we're going to see that. Let us see a few examples. We have two operations here, an operation's in the increment method, which is a write operation, the incrementation of the index variable. And the read operation in the method print which is simply the writing of this index variable on the console. What does this code prints in multithread? That is if the incrementation is executed in a given thread, T1, and the printing in another thread, T2. Well, this code, as it is written here, without any synchronization and no volatile declaration, the answer is it is impossible to say. The printing of the index variable is not bound to the last write operation, so we can hope so really any kind of value printed out. Now, if we synchronize our two methods on the same objects, which is the instance in which those two methods have been declared, then we have a synchronized write and a synchronized read, so we have a "happens before" link between our write and our read operation. So the correct value is always printed, the value printed by the System. out. println is the last value updated by our write operation. Now, if I do not synchronize my code but declare my variable index as being volatile then the write and the read operations are volatile read and writes, and the correct value will also be always printed out.

Understanding the Happens-before Link on a Complex Example
Let us see now a more complex example. We have a method called firstMethod with two write operations. The first x = 1 unsynchronized and non-volatile since x is not declared as being volatile, and y = 1, which is a synchronized write. And the secondMethod with two read operations, the first one, r1 = y, synchronized read, and r2 = x, non-synchronized read. And the question is, if I execute this code in two different threads, the first one executing firstMethod and the second one executing secondMethod, what the value of r2 is going to be? Due to the way the code is written there is a "happens before" link between the x = 1 and the y = 1 operation. Why? Because those two pieces of code are written in the same method, and, of course, a given thread is supposed to respect the order in which I have written my code. So I have a "happens before" link between x = 1 and y = 1, and on the other hand, between r1 = y and r2 = x. Let us suppose at first that T1 is the first thread to enter the synchronized block. So we know that we have a "happens before" link between the write operation y = 1 and the read operation r1 = y. Since I also have a "happens before" between x = 1 and y = 1, and r1 = y and r2 = x, I know that the execution will be in this order: first x = 1, then y = 1, executed in my first thread, and r1 = y and r2 = x in my second thread. And the value of r2 is 1. Why? Because by transitivity I have a "happens before" link between x = 1 and r2 = x. But on the other hand, if T2 is the first thread to enter the synchronized block then things are not going that well because I have a "happens before" link between r1 = y and y = 1, but since r2 = x and x = 1 are not synchronized or volatized read and write operations I have no "happens before" link between those two operations, and I cannot know if x = 1 will be executed and visible before r2 = x. So the value of r2 may be 0 or 1 and it is not possible to say this in advance. So, from a concurrent programming point of view, this code is buggy because I cannot tell in advance what value r2 will receive.

Synchronization and Volatility on Shared Variables
As a conclusion, on one hand we have synchronization. Synchronization guarantees the exclusive execution of a block of code, only one thread can execute a special block of code at a given time. And we have visibility, that guarantees the consistency of the variables. If a variable is visible then I have the guarantee that when I read it I read a correctly updated value. Visibility is a weaker constraint that synchronization because two threads can execute the read and the write operations at the same time but I still have the consistency of the variables guaranteed. So the conclusion of this is that all shared variables, and what does "shared" means? It means shared among more than one thread. Should be accessed by those threads either in a synchronized way, either in a volatile way. If you see a variable in a piece of code that is read or written by more than one thread at the same time and that is neither synchronized nor volatile then you have a race condition and a bug in your code.

Understanding False Sharing on Multicore CPUs
Let us now talk about false sharing because this structure of caches inside the modern CPU has a drawback which is precisely called false sharing. What is false sharing? False sharing happens because of the way the CPU caches work. It is a side effect, unfortunately it can have a tremendous effect on performance, so it is good to understand what it is and to have that in mind when designing applications. In fact, the cache of a CPU is organized in lines of data. Each line can hold 8 longs, which is 64 bytes. And when a visible variable is modified in an L1 cache, all the line is marked "dirty" for the other caches. A read on a "dirty" line triggers a refresh on the whole line, not just on the variable that has been modified. Suppose we have this very simple code, a class with two fields, volatile longs a and b, and a firstMethod that is incrementing "a" and a secondMethod incrementing "b". The first thread is running firstMethod and a second thread running secondMethod. So the first thread is only interested in variable "a" and the second thread only in variable "b". The first thread is running in Core 1, and since it needs the variable "a" it loaded a line of cache from the main memory with this variable in this line. And the second thread did the same, loaded a line of cache from the main memory with the "b" variable in it. Now, because of the way the memory is organized in our application, organized by the compiler and the JVM, it turns out that "a" and "b" are written in two contiguous areas of the main memory. So, while loading this line of cache T1 also loaded the "b" variable and T2 also loaded the "a" variable.

How False Sharing Can Impact the Performance of Applications
So, what is going to happen now? Well, the thread T1 is going to increment the "a" variable, thus marking this line of cache as dirty, and this mark as dirty will be broadcasted to the other caches of the CPU, including the cache of Core 2. Then Core 2 wants to increment the "b" variable, but unfortunately the line of cache it loaded from the main memory has been marked as dirty by Core 1, so when it tries to read the variable "b" it is a cache miss, it has to go back to the main memory to fetch the value of "b" it is going to increment. Which is really bad luck because the variable "b" has not been touched by Core 1, the "b" variable has been made dirty by the side effect of the fact that the CPU cache is organized in lines. This is a very well-know drawback of the cache organization called false sharing. False sharing happens in our application, in our code, in a completely invisible way, because when we write some code and when we write a class, we have no idea of how the class and its field are laid out in memory. It is hard to predict, but it is with no doubt hitting the performances of our applications. There are workarounds to prevent false sharing from happening in very simple cases, and we are going to see an example of that.

Live Coding: Setting up a Simple Example to Observe False Sharing
Okay, time for a little live coding session. What are we going to see in this live coding session? Well, we are going to see some Java code in action, of course, as usual, and this time it will be an example of false sharing artificially created in a piece of code, and the influence of variable padding to try to fix this. We are going to see how false sharing can really kill the performance of an application and how variable padding can fix this problem. Let us talk now about false sharing, and let us build an artificial example of a class that is working using false sharing. We are going to run some code in multithread so we have a constant here that will fix the max number of threads we are going to use. And since it is a very fast operation we are going to run our atomic operation a certain number of times, here 50 million times. So, we have a first class called VolatileLongPadded and a second one, VolatileLongunPadded. What does this class do? In fact it doesn't do anything, it just rubs a long value that is set to 0. So, if we create several instances of this class most probably they will be recorded on the same cache line of the CPU thus sharing this cache line and probably expose the false sharing problem we have identified in the slides. And we have a second class which is basically the same, it also rubs long value here with the value 0, but just to be sure that we will not come across some false sharing we have created six other long before this long value, and six more long values after it. So here we have 13 long, thus we are sure that this value will be isolated in a cache line, and if we create several instances of this VolatileLongPadded class we can be sure that this volatile field here will not share the same line of cache with another instance. So let us use these classes to create arrays of padded longs and unpadded longs. And let us run the following code which is here.

Live Coding: Observing False Sharing on a Simple Example
We are going to test this code several times. First with only one thread to have some kind of ground truth with no false sharing, then with two threads, three threads and four threads. What do we do here? We have a first run here that creates the right number of threads with a certain runnable that we are going to see, start those threads, join them to be sure that they have finished their work, and print out the results. And we do the same with another runnable returned by this method which is called createUnpaddedRunnable. So basically for one, two, three and four threads we launch the same kind of computations, one time with a runnable created by this createPaddedRunnable, and another runnable created by this createUnpaddedRunnable. Let us have a look at those runnables. The first one will simply touch the unpadded long value, it will modify your value that most probably shares the same line of code with other values. And in the second case we have the exact same runnable but this time we are modifying the value in the paddedLongs object that is protected against false sharing. Let us now run this code. We can see that when we have only one thread the performances are almost the same, 300 milliseconds. When we have two threads we see that in the unpadded case we already have an impact on the performances and this impact will raise as the number of thread increases. So clearly on this example we can see that the false sharing is there, and that variable padding can protect our code from this false sharing. Now, is this variable padding trick, should be used all the time? Most probably not, this is just a toy example, you can probably reproduce it very easily in your machine, but on real applications it will probably be different.

Wrap-up of the Module
Okay, so now is the time to wrap up this module. What did we see in this third module? Well, we saw the importance of visibility in multicore CPU, and this is really a key point to understand concurrent programming in concurrent application design. Visibility is a problem that came with multicore CPU and that was not there when we had monocore CPU to implement our applications. We saw two fundamental notions in concurrent applications, which is synchronization and volatility. Those two notions are also very important to understand. Synchronization is about atomicity, that is only one thread can execute a synchronized block of code at a given time. And volatility is about visibility, and of course visibility of a variable through the caches of the different calls of a given CPU. And we also saw how the way caches work in multicore CPU can generate the problem of false sharing. We saw how we can pad variable to prevent this problem to occur, and we also saw that it is not a very good solution, at least not a solution we can use everywhere. Thank you for your attention in the third module of this course, now you can bear with me with the fourth module will be a use case study around the problem of the Singleton pattern. We already talked about the Singleton pattern, we are going to analyze very precisely this pattern, and show how it can be implemented efficiently and correctly in the Java language.

Implementing a Thread Safe Singleton on a Multicore CPU
Introduction: Agenda of the Module
Hello, my name is Jose. Welcome to the fourth and last module of this course called Implementing a Thread Safe Singleton on a Multicore CPU. This module is entirely devoted to a case study based on the well known Singleton pattern, and we're going to see how we can implement the Singleton Pattern in a thread safe and efficient way on a multicore CPU. We're going through the different elements we saw on this course, synchronization, locking, visible read and write operations, and happens before links. Let us first quickly browse through the agenda of this module. First we are going to see once again what is the Singleton pattern, very quickly because it is a very common pattern, and we already talked about it at the beginning of this course. Then we are going to try to implement this Singleton pattern in a thread safe way, and of course the first solution that comes to mind is to synchronize the excess of the Singleton variable, but we're going to show how synchronization leads to poor performances even if it works perfectly well from a purely functional point of view. And then we are going to talk about a solution that has been brought in some technical articles called the double-check locking, and if we're going to talk about this double-check locking, it's for two reasons. First it might look like a good solution, a solution that works and that solves the problem of the non-concurrent read operation on the Singleton variable, but also because this pattern is in fact buggy. It is a very subtle bug, very hard to detect, even harder to observe in a working application, but it is still a bug so the double-check locking should not be used on a production environment. And then we will see the right solution, the right implementation of the Singleton pattern in Java, which is extremely simple and based on the well known enumerations.

A First Implementation of the Singleton Pattern
Let us jump in the first part of this module which is the Singleton pattern. As we already saw it, the Singleton pattern is a well known pattern described in the famous design pattern books called the Gang of Four. The idea is that a Singleton class should have only one instance. It is very easy to write in a non-concurrent environment, in a non-multi threaded application, but if my application is multi threaded, if I am in a concurrent environment, I need to be careful. If I am not I might end up with a Singleton with multiple instances. Let us have a look at the first implementation of the Singleton pattern in Java, a quite naive implementation because as we are going to see it, it is not thread safe. Well, we need a static field to store the only instance of the Singleton class, your private static Singleton instance. We want to prevent outside code from being able to instantiate this class, so an easy way to do that in Java is to make the constructor private, so that this constructor cannot be called from outside this class, and final, this will prevent this class from being extended. Of course we still need a way to get an instance of this class from outside of this class, so we need a static method that can be called without any instance, public static Singleton getInstance. What is this getInstance method doing? Well, first it checks if the instance field is null, if it is then it creates a new instance of Singleton and stores it into this field and returns this field, thus guaranteeing that no two instances of the Singleton class are going to be created, at least in a mono thread environment.

Identifying the Race Condition in This First Implementation
Alright, so what are the problems with this code? First we have a read operation here in green, instance is equal to null, and a write operation here in red, instance equals new Singleton. If those two operations occur in different threads I have here a good example of a race condition, a read and a write operation occurring at the same time in two different threads. I have no happens before link between those two operations, so I have no guarantee that our first thread has created a new instance, and that the second thread will see this new instance. This first implementation is not thread safe, which is not a surprise, we already knew that. The first solution that can come to mind is to make the read and the write operation synchronous, that is making this getInstance method synchronous.

Fixing the Race Condition Using Naive Synchronization
This is the first fix to this problem called the synchronized Singleton pattern. It is simply written like that, just by adding the synchronized keyword on the declaration of the getInstance method. What is this fix going to do? Well, it is just going to prevent two threads from executing this getInstance method, so we have the guarantee that only one instance of the Singleton class is going to be created. From a purely functional point of view, this fix is perfect, it will make our Singleton class work as expected, even in a multi threaded environment. Let us have a closer look at the execution of this code of a Single Core CPU. Suppose that two threads T1 and T2 are calling the getInstance method at the same time. T1 is the first to enter the synchronized block, let us suppose that, and let us see what is going to happen.

Analyzing the Performance on a Single Core CPU
Let us have a closer look at what is happening at the CPU level. At the core level T1 is entering the getInstance method and executes a test. This is happening first on the timeline, then at some point the thread scheduler gives the hand to T2. Here T2 is going to try to enter a getInstance method, but since the key is not available, T1 is still holding it, it will realize that it cannot do that, but the thread scheduler will give the hand to T1 again very quickly so T1 can finish it's execution of the getInstance method and return with the newly, freshly built instance object. Then the thread scheduler will give the hand back to T2 so that T2 can enter the getInstance method, read the instance just created in T1, and return with it, since the write to this instance object was a synchronized write under the read made by T2, is a synchronized read T2, has the guarantee to read the correct value of instance.

Analyzing the Performance on a Two Core CPU
Let us have a look at what is happening on a two cores CPU. Here T1 is entering getInstance, and executing the test on the first core of the CPU. The thread scheduler knows that it has two cores on it's ends, so it is going to give the hand to T2 on the second core of my CPU, at the same time T1 is running. T2 tries to enter the getInstance method. Of course it cannot because T1 is in it, holding the key of the synchronized block. But something different is happening here because I am on a two cores CPU. What is different is that T2 knows that some other thread is running on the other cores so there is a little chance that the key might be released without T2 leaving the core of my CPU, so it is going to wait a little for this key to be released. There is a little time out here running. At some point it will realize that the key is not released, so maybe the thread scheduler will give the hand to another thread in my application. T1 will finish to execute getInstance, T2 can then enter the getInstance method with the key, read the instance. Once again this is a synchronized write followed by the synchronized read, so everything is fine. T2 is going to read the correct value of the instance variable.

Understanding the Performance Issue of the Synchronized Singleton
But something weird happened, because suppose I have not two cores on my CPU, but four cores. I saw that T1 was running first, then T2 needs to wait for T1 to leave the synchronized method to be able to read instance. And if on my two other cores I have two other threads, T3 and T4 who want also to read my instance variable, well those threads will have to wait for T2 to leave the getInstance method. At this point instance has been created, so all the reads could happen at the same time, the exact same time, but since my method is synchronized, no more than one thread can enter it at a given time, so no more than one thread can read instance at the same time, and this is really a performance hit because the more cores I have the more time I am going to lose since the reads cannot be made in parallel. Of course all the reads could be made in parallel in a correct way, but the synchronization of the method does not allow that. In the case of an execution on a multi core CPU, since the read is synchronized it cannot be made in parallel. Once instance has been initialized I want to be able to allow the reading of this variable in parallel in a correct way. This synchronization of the Singleton pattern used to be a good idea in a mono core CPU world, now that we are in a multi core CPU world, it is not the right pattern to use because it is not performant enough.

Fixing the Race Condition Using Double Check Locking
The first solution that has been proposed to fix this performance problem was the double check locking Singleton pattern. We're going to see this pattern in detail, and see why it is bugged, and why of course it should not be used. The fact is I do not want to synchronize my getInstance method because it prevents the reads to be conducted in parallel, so let us remove this synchronized keyword here, and let us think again at another way of synchronizing the creation of this instance variable. The idea of the the double check locking is the following. Let us first check inside the getInstance method if the instance field has been created or not. If it has been created then I just return it, and this is fine because it is not in the synchronized block then all my reads will be made in parallel. And then if instance has not been created I have the synchronized block on a special key object which will be static of course, and inside this I have a classical code. Check if instance is null or not. This is a good idea because between the first test and this one another thread could have created this instance of Singleton. If it is null then I create it and return this value. This seems to be a nice way of doing things, but there is a bug in this code which is very subtle, and that we are going to see now.

Understanding the Concurrent Bug in the Double Check Locking
It looks like the double check locking is solving our reading problem. Is it really solving this problem? Let us take a closer look at this code. The first operation is if instance is not null, then we read it and return it. Here we have obviously a read operation on the instance field of the Singleton class. Is it a synchronized or volatile read? Well, the answer is no. Instance is not a volatile variable, and this test is not in a synchronized block. We have written it outside of the synchronized block on purpose so it is not inside this synchronized block, so it is not a synchronized read neither. Then if instance is null we create it, and return it, so obviously here there is a write operation. Is it a synchronized or volatile write? Well it is not a volatile write because instance is not a volatile variable, but it takes place in a synchronized block, so it is a synchronized write, the answer is yes. We have a non synchronized read that is supposed to return the value set by a synchronized write. Do we have a guarantee that the read will get the value set by the write operation? Well, we saw that in the previous module. This either happens before link question. For that to have this guarantee, we need a happens before link between this write operation and the read operation outside of the synchronized block, and the fact is we do not have this happens before link because happens before links are created between synchronized or volatile writes and synchronized or volatile reads. So this code is buggy because there is no happens before link between the read returning the value, and the write that is setting this value. This is a very subtle bug indeed, it is not very obvious to see this bug, but this bug is here indeed.

Possible Issues with the Double Check Locking
The double check locking is buggy. There is concurrency bug in the way the double check locking is written. It is a very subtle bug because you will never observe this bug on a single core CPU because there are no visibility issues on a single core CPU. Visibility issues can only be seen on multi core CPUs due to the different caches on each core of the CPU. And in fact the effect of this buggy double check locking can be very weird. One can observe an object that is not fully built. To understand this let us have a closer look on how this instance field is created. In our Singleton class we have a private static field called instance of type Singleton. It is just a pointer that will point to our only instance of the Singleton class. But the creation of this object is in fact a two step process. The first step is memory allocation as in all the languages, then the copy of the pointer that points to this newly allocated memory into the Singleton field which is called instance, and then the construction process of the Singleton object. This allocated memory belongs to the Singleton class, it has a certain number of fields, certain number of methods etc. And between the B step and the C step, we do not know which one is going to be executed first. If the construction process is executed first, then the the copy of the pointer, everything will be fine because we cannot observe a non null instance pointer that points to a non fully built piece of memory, but if the copy of the pointer occurs first, and that we read this instance field in another thread, then we will have visibility on a portion of memory that has been allocated but is not fully initialized yet, and this weird case could happen in the double check locking we do not have the guarantee that this second case will not happen, and if it does then very bad things can happen to our applications because basically what we can do is call methods on an object that is not fully built, that is completely corrupted at this step. Of course this is something that should be avoided in our applications.

Fixing the Race Condition the Right Way: Using Enumeration
It is not possible to use this double check locking and the bug comes from the fact the read of the instance variable is neither synchronized nor volatile. A possible solution could be write, let us make it a volatile read so that we get rid of this problem. And the code will be this one. This time the Singleton instance is a volatile field. We have a synchronized write followed maybe in another thread by a volatile read so everything is fine. From a purely functional point of view this code will work perfectly well. Indeed in this case the double check locking is fixed, the problem is that we will fall back on the same kind of performance issues as in the synchronized case, so in fact we have fixed our problem without really fixing it since we have the same performance issues. What is the right solution to this Singleton pattern in Java? We didn't find the right solution is to use an enumeration just like that. It is also a very simple solution, very easy to read and to write and to understand, so if you have Singletons to implement, forget about the basic Singleton pattern. Do not forget to rub the double check locking because you absolutely want to keep in mind that this pattern is buggy, so do not forget that you should avoid the double check locking at all costs. This pattern is so simple that it might look suspect, but in fact it has been used in several places in the JDK. For instance in the Comparator Interface in Java 8 where we have this static method called Natural Order that returns a comparators. naturalordercomparator. instance and what is this Natural Order Comparator stuff? It is indeed an enumeration that implements Comparator, that has only one instance called instance with the implementation of the two methods compared and reversed for some reason, so yes this pattern based on enumeration can be used. It is in fact the right pattern to use to implement Singletons in Java.

Live Coding: Trying to Fix the LongWrapper Example with Volatile
Now is the time for our live coding session. What are we going to see in this live coding session? Well, once more we are going to see some Java code in action which is great. We are going to see volatility in action, and for that we will come back to the first example of the first module of this course, and see if volatility could have helped us to fix our race condition problem, and also we're going to fix the example of the first module, because now that we saw the problems with the double check locking, the problem with volatile writes, non volatile reads and things like that. A second look to our first example will show us some bugs that we left. Let us come back to the first example of the first module of this course, and let us take a closer look at what we have written during this live coding session. Basically we had a long wrapper, with several threads writing and reading this long wrapper and to fix the race condition problem we just synchronized the incrementation of the wrap value from this long wrapper. The first question I would like to see is would it have been correct to just declare this n-variable as a volatile longer, just like that. Let us comment out this key object, comment out the synchronized declaration, and see if it works. Obviously this value should be one million, it is not the case so this fix is not working. Why isn't it working? Because here I have a volatile read and the incrementation and then a volatile write, so this value will get the correct value created here. But the problem is not this one, the problem is that this operation should be atomic. It should not be interrupted when a thread is running it by another thread, so what we need here is not just volatility, it is really synchronization to guarantee that the thread will not be interrupted between the reading and the writing of this variable, so yes indeed synchronization was the correct answer, volatility is not enough.

Live Coding: Fixing the LongWrapper Example with Synchronization
But if we take a second look at this code, we might note that in fact this code is buggy and it is a very subtle bug once again because this code runs correctly on my IDE, that is in the development process. Of course I do not have that many threads, I am not in the real concurrency context that I can have in a production environment, but let us take a closer look at this one. What do I have here? I have a synchronized read followed by a synchronized write which is correct. This block of code will correctly increment my variable, but this getValue here is neither synchronized, neither volatile, so this getValue here does not offer the guarantee of returning the last value returned in this synchronized block. If I want this code to be completely correct, I need to make this read also synchronized just by putting it in a synchronized block like that. Of course the result will be the same but the difference is that now this code is correct. It was not correct in the first version, why? Because I did not have the happens before link between this write and this read.

Live Coding: Presenting the Java Puzzler LockMess Case Study
Let us have a look now at one last example. This example will show us several things that we learnt during this course. This example is taken from the famous book by Neal Gafter and Josh Bloch called Java Puzzlers which is an excellent book worth reading. We have a class that extends thread. In this class there is a main method, what does it do? It just called the start method of the thread that will in turn call the run method of a redone in that class. In this run method there is basically a while called here that will keep running until the quitting time is set to true. Here we are working, while we are working we keep printing 'still working' and once our work is done just coffee is good because it's probably the time to get a good cup of coffee. And in the working method there is just the thread. sleep called that will wait for 300 milliseconds, great! The problem is that quitting time will never be set to true so we need a way to set this variable to true just to quit working, so let us have a look at the rest of the code. We set up a timer. A time is a special object that will take a timer task as a parameter. It is basically the same as a runnable and calling this schedule method will just run this task once after 500 milliseconds which is the second argument of this schedule method. Here after 500 milliseconds we are calling worker. keepWorking, and this keepWorking method which is here is synchronized and sets quitting time to false, and then after 400 milliseconds we call worker. quit. What does this quit method do? Well, this method is here. It is also synchronized and synchronized on the same object as the keepWorking method so if a thread is executing this code, another thread cannot execute this one at the same time, and in the quit method I set quitting time to true, then join the thread, that is wait for the thread to complete before releasing the key. Basically since I am calling this method after 400 milliseconds, I do not expect this timer task to reset the quitting time to false. What I expect in this code is that I should have still working printed, then coffee is good and then this timer will run but after the worker thread is completed. Let us run this code and we can see that this is not quite what we expected since it seems that our system is still working and never quitting, so let us shut this down and let us try to understand what is happening.

Live Coding: Analyzing the Bug in the LockMess Case Study
We would like to be sure that this quitting time equals true is properly called, and that this quitting time equals false is never called. In fact this quitting time equals false should not be called before this red has finished to execute the quit method, but it seems that it is not exactly the case, so let us add some code in here to try to understand what is happening really. Let us add a system that outs. println just before the join call and then just after this join call. And let us do the same, let us add a system out println here in the keepWorking method, and let us now run this code once again, and we can see that something really weird is happening because we're still working, calling join and keepWorking here. So it seems that our thread here is blocked on the join call here after having printed calling join. And let the other thread execute this code and print out keepWorking. Why is this thread blocked on the join? It seems that the other one could take the key of the synchronized block and execute this synchronized block. Let us have a closer look at this join method here. This join method is a public final void method here, that just called another join method that takes zero as an argument. Let us see this other join method. There are several things in it. First, it is a synchronized method, while the join method was not synchronized. Since locks are re-entrant in Java and we saw that in our previous module, we can call this join method because we are holding the write key, but inside this method we can see that there is a call to the wait method of course of the object we're synchronized on. And we also saw in the previous module about the wait notify pattern that this wait call will release the key that is held by the thread, so in fact here this join call releases the key held by the thread in this synchronize method, letting another thread to execute this code here. This code is not doing what we want it to do, just because there is a lock mess here. Something went wrong since this call releases the key our thread is holding.

Live Coding: Fixing the Bug in the LockMess Case Study
How can we fix this problem? Well there's a very simple solution, we can just change the key we are holding by creating a special lock object. Let us do that, private Object lock equals new Object, and let us synchronize this code here on this new lock object, and do the same in the keepWorking method. This time we have a double synchronization, but this block of code is synchronized on a lock object that is hidden in our class and not visible from the outside. Let us run this code and we can see this time that our system is working as expected. What we should learn from this example is in fact two things. First never suppose anything on how an external call is dealing with the key of our synchronization, always check for that because you might have surprises as is the case for this join method here. And the second thing we can see is that you will never have any problem if you use private Object to synchronize your block of code. Never expose to the outside the object you use to synchronize your code. This is really a golden rule in concurrent programming.

How to Write Correct Concurrent Code Wrap-up
Before wrapping up this course what I would like to do is to give you a simple step by step procedure to check if your concurrent code is correct. First you need to check for race conditions, so you need to have a look at you code and especially what is happening to the fields of your classes because race conditions cannot occur on variables inside methods nor parameters. If you have more than one thread trying to read or write a given field then it means that you have a race condition on that field. It is in fact as simple as that. Then you need to check for the happens before link. On this given field if you want things to be correct you need to have a happens before link between your read operations and your write operations. It is quite easy in fact to check for these points. They have two questions you need to answer for that. First are your read or write operations volatile? Second question, are they synchronized? If the field you are checking has been declared volatile they are synchronized if they occur inside the boundary of a synchronized block, so it is very simple to check for that. If it is not the case you must probably have a bug. Remember the double check locking bug, those bugs can be very subtle, very sneaky. And there is a side question you might also want to ask yourself. Should I go for read or write operations that are synchronized or volatile? There is no simple answer to this question. You need to answer another question in fact to answer this one which is, do I need atomicity on a certain portion of code? If you have a certain portion of code that should not be interrupted between threads, then you need to have a synchronized block to protect this portion of code. If it is not the case then volatility is enough. It will ensure visibility and correct concurrent code.

Course Wrap-up and Thanks!
Now is the time to wrap up this last module of this course. What did we learn in this module? We studied thoroughly the Singleton patterns, even in it's sophisticated double check locking version, and we saw how very subtle bugs can arise in concurrent programming. Really if you want to hand down those bugs, you need to understand what is the happens before link and check for the existence of this link between your write and your read operations. The only way to find those bugs is really to rely on a very good understanding of what is this happens before link, and to check for the existence of this link between all your read and write operations. Checking this is in fact not that complicated. There is one question to ask ourselves which is, are those read and write operations synchronized or volatile? And it is fairly simple to check that. It is fairly simple to see if we are in a synchronized block or to see if the variable we are reading and writing is a volatile variable or not. If this happens before link does not exist then race conditions can occur. The bugs created by race conditions are rarely seen in the IDE during the development process, and they are most of the time seen in production environment and of course this is something we do not want to observe. And with this I think we are done with this course, thank you for watching it, thank you for your attention, and I will be very glad to interact with you in the comments of this course if you need it.